{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with ResNet50 for Plant Disease Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "We import the main libraries used in this notebook: TensorFlow/Keras for the CNN, NumPy for arrays, and Matplotlib for plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic configuration\n",
    "Set the key hyperparameters: image size, batch size, number of channels (RGB), and training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256) # 256x256 pixels\n",
    "batch_size = 16 # images per batch\n",
    "channels = 3 # RGB\n",
    "epochs = 50 # number of training epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the dataset and exploratory data analysis \n",
    "Loads images from your `PlantVillage` directory. Each subfolder name becomes a class label automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from a folder structure into a tf.data.Dataset.\n",
    "# Expected folder layout:\n",
    "#   ../data/raw/PlantVillage/\n",
    "#       class_1/  (images...)\n",
    "#       class_2/  (images...)\n",
    "#       ...\n",
    "# Each subfolder name becomes the class label automatically.\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory = \"../data/raw/PlantVillage\",     # root folder containing one subfolder per class\n",
    "    image_size = (image_size, image_size),      # resize every image to this fixed size (H, W)\n",
    "    batch_size = batch_size                     # how many images per batch returned by the dataset\n",
    ")\n",
    "\n",
    "# Result:\n",
    "# - dataset yields batches of (images, labels)\n",
    "# - images shape: (batch_size, image_size, image_size, 3)\n",
    "# - labels are integer class IDs (e.g., 0..num_classes-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class names\n",
    "Keras assigns an integer label to each class based on folder order. `class_names` stores the mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of class (label) names inferred from the subfolder names\n",
    "# inside the PlantVillage directory.\n",
    "# Example: [\"Apple___Black_rot\", \"Apple___healthy\", ...]\n",
    "class_names = dataset.class_names\n",
    "\n",
    "# Display / print the class names (in notebooks, the last line shows the value)\n",
    "class_names \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one batch\n",
    "Checks the tensor shapes coming from the dataset and prints the label ids in that batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    print(image_batch.shape)   # image batch shape\n",
    "    print(label_batch.numpy()) # labels as numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect one image shape\n",
    "Shows the shape of a single image tensor (height, width, channels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    print(image_batch[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect raw pixel values\n",
    "Prints raw pixel values for one image (before rescaling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    print(image_batch[0].numpy())  # pixel values (array) of the first image in the batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a sample image\n",
    "Displays one example image from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in dataset.take(1):\n",
    "    plt.imshow(image_batch[0].numpy().astype(\"uint8\"))  # show first image (convert to uint8 for display)\n",
    "    plt.axis(\"off\")                                      # hide axes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label id → class name mapping\n",
    "Prints the numeric label id for each class. **Note:** this cell previously contained extra code; it has been cleaned to only show the mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_id, class_name in enumerate(dataset.class_names):\n",
    "    print(f\"{label_id} : {class_name}\")  # print: class_index : class_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect one sample per class\n",
    "Unbatches the dataset and stores the first image seen for each class. This is useful for later visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class_names = dataset.class_names                 # class names (from folder names)\n",
    "num_classes = len(class_names)                    # number of classes\n",
    "\n",
    "samples = {}                                      # store 1 sample image per class\n",
    "\n",
    "for img, label in dataset.unbatch():              # iterate image-by-image (not in batches)\n",
    "    label_id = int(label.numpy())                 # tensor -> int\n",
    "    if label_id not in samples:\n",
    "        samples[label_id] = img                   # keep the first image for this class\n",
    "    if len(samples) == num_classes:\n",
    "        break                                     # stop after collecting all classes\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(num_classes / cols)              # rows needed for the grid\n",
    "\n",
    "plt.figure(figsize=(4*cols, 4*rows))\n",
    "for i, label_id in enumerate(sorted(samples.keys())):\n",
    "    ax = plt.subplot(rows, cols, i + 1)           # position in the grid\n",
    "\n",
    "    img = samples[label_id].numpy().astype(\"uint8\")  # convert to display format\n",
    "    plt.imshow(img)                               # show image\n",
    "    plt.title(f\"{label_id} → {class_names[label_id]}\", fontsize=10)  # label + name\n",
    "    plt.axis(\"off\")                               # hide axes\n",
    "\n",
    "plt.tight_layout()                                # nicer spacing\n",
    "plt.show()                                        # render the figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/validation/test split\n",
    "Splits a `tf.data.Dataset` into train/val/test using `take()` and `skip()`. This assumes the dataset has a known length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliting_the_data(ds, train_split=0.80, val_split=0.10, test_split=0.10, shuffle=True, shuffle_size=1000):\n",
    "    ds_size = len(ds)                          # total number of batches/elements in ds\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12) # shuffle before splitting\n",
    "\n",
    "    train_size = int(train_split * ds_size)    # number of items for train\n",
    "    val_size   = int(val_split * ds_size)      # number of items for val\n",
    "\n",
    "    train_ds = ds.take(train_size)             # first part -> train\n",
    "    val_ds   = ds.skip(train_size).take(val_size)  # next part -> val\n",
    "    test_ds  = ds.skip(train_size).skip(val_size)  # remaining -> test\n",
    "\n",
    "    return train_ds, val_ds, test_ds           # return the 3 datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the splits\n",
    "Runs the split function and produces `train_ds`, `val_ds`, and `test_ds`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = spliting_the_data(dataset)  # split dataset into train/val/test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check split sizes\n",
    "Shows the number of batches in each split (may be `unknown` in some pipelines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds), len(val_ds), len(test_ds)  # number of batches/items in each split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing and data augmentation\n",
    "- **Resizing** ensures all images have the same shape.\n",
    "- **Rescaling** normalizes pixels to `[0, 1]`.\n",
    "- **Augmentation** creates random flips/rotations to improve generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ResNet50 transfer learning, we keep the pixel range 0..255 (no 1/255 scaling),\n",
    "# because tf.keras.applications.resnet50.preprocess_input expects that range.\n",
    "\n",
    "resize_only = tf.keras.Sequential([\n",
    "    layers.Resizing(image_size, image_size),   # resize to fixed size\n",
    "], name=\"resize_only\")\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class ResNet50Preprocess(layers.Layer):\n",
    "    # Serializable preprocessing layer for ResNet50\n",
    "    def call(self, inputs):\n",
    "        # inputs expected in RGB with values in [0, 255]\n",
    "        return preprocess_input(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "resnet50_preprocess = ResNet50Preprocess(name=\"resnet50_preprocess\")\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal_and_vertical'),  # random flips\n",
    "    layers.RandomRotation(0.2)                     # random rotation\n",
    "], name=\"data_augmentation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the ResNet50 model (transfer learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture (ResNet50 backbone + custom head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "n_classes = len(class_names)\n",
    "input_shape = (image_size, image_size, 3)\n",
    "\n",
    "# ✅ ResNet50 backbone (no name= argument)\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "# ✅ IMPORTANT: ResNet50 expects its own preprocessing (not /255)\n",
    "preprocess = tf.keras.applications.resnet50.preprocess_input\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    resize_only,                                  # your resize layer\n",
    "    layers.Lambda(preprocess, name=\"resnet50_preprocess\"),\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(n_classes, activation=\"softmax\"),\n",
    "], name=\"ResNet50_transfer_learning\")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "Sets the optimizer, loss function, and metrics. `SparseCategoricalCrossentropy` is correct when labels are integer-encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',   # training algorithm (Adam optimizer)\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),  # loss for integer labels + softmax output\n",
    "    metrics=['accuracy']  # track accuracy during training/eval\n",
    ")\n",
    "\n",
    "model.summary()  # show model architecture + parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # training callbacks\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",              # watch validation loss\n",
    "        mode=\"min\",                      # lower is better\n",
    "        patience=9,                      # stop if no improvement for 9 epochs\n",
    "        restore_best_weights=True        # keep best weights found\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\",              # file to save best model\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_best_only=True              # save only when val_loss improves\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        factor=0.5,                      # reduce LR by half\n",
    "        patience=2,                      # wait 2 epochs without improvement\n",
    "        min_lr=1e-6,                     # don't go below this LR\n",
    "        verbose=1                        # print when LR changes\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU setup (optional)\n",
    "Enables memory growth so TensorFlow doesn't reserve all GPU memory at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # TensorFlow library\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')          # list available GPUs\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True) # use GPU memory as needed (avoid full pre-allocation)\n",
    "\n",
    "print(\"GPUs:\", gpus)                                   # show detected GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train or load a saved model\n",
    "Use `TRAIN = True` to train and save, or `TRAIN = False` to load the saved model from disk. This cell also prints TensorFlow/Keras versions and some file info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 232/2348 [=>............................] - ETA: 1:12:53 - loss: 0.2034 - accuracy: 0.9351"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     model_history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# training dataset\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# validation dataset\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# number of epochs\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# callbacks (early stop, checkpoint, etc.)\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# show training progress\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Fine-tune (optional)\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m FINE_TUNE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1800\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1801\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1804\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1805\u001b[39m ):\n\u001b[32m   1806\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1809\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    829\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    835\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    865\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    866\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    872\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1319\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1321\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1322\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1325\u001b[39m     args,\n\u001b[32m   1326\u001b[39m     possible_gradient_type,\n\u001b[32m   1327\u001b[39m     executing_eagerly)\n\u001b[32m   1328\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1484\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1494\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1495\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1496\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1501\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data_science_dir\\image_clasification_model_with_streamlit_deployment\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os                                   # file/path utilities\n",
    "from datetime import datetime                # (not used here, can remove)\n",
    "import tensorflow as tf                      # TensorFlow\n",
    "\n",
    "MODEL_PATH = \"../models/ResNet50_image_classification_model.keras\"  # where to save/load the model\n",
    "TRAIN = True                               # True = train + save, False = load existing model\n",
    "\n",
    "# Optional: fine-tune the top layers of ResNet50 after the first training stage\n",
    "FINE_TUNE = False                            # set True only after you see stable val accuracy\n",
    "FINE_TUNE_EPOCHS = 10                        # extra epochs for fine-tuning\n",
    "UNFREEZE_LAST_N = 30                         # unfreeze the last N layers of the ResNet50 backbone\n",
    "\n",
    "if TRAIN:\n",
    "    print(\"Training model...\")\n",
    "    model_history = model.fit(\n",
    "        train_ds,                            # training dataset\n",
    "        validation_data=val_ds,              # validation dataset\n",
    "        epochs=epochs,                       # number of epochs\n",
    "        callbacks=callbacks,                 # callbacks (early stop, checkpoint, etc.)\n",
    "        verbose=1                            # show training progress\n",
    "    )\n",
    "\n",
    "    # Fine-tune (optional)\n",
    "    if FINE_TUNE:\n",
    "        print(\"Fine-tuning the top layers of ResNet50...\")\n",
    "        backbone = model.get_layer(\"resnet50\")\n",
    "        backbone.trainable = True\n",
    "\n",
    "        # Freeze early layers, unfreeze only the last UNFREEZE_LAST_N layers\n",
    "        if UNFREEZE_LAST_N is not None and UNFREEZE_LAST_N > 0:\n",
    "            for layer in backbone.layers[:-UNFREEZE_LAST_N]:\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Keep BatchNorm layers frozen for more stable fine-tuning\n",
    "        for layer in backbone.layers:\n",
    "            if isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = False\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        fine_tune_history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=FINE_TUNE_EPOCHS,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    model.save(MODEL_PATH)                   # save trained model\n",
    "    print(\"Saved model to:\", MODEL_PATH)\n",
    "\n",
    "else:\n",
    "    if os.path.exists(MODEL_PATH):           # check model file exists\n",
    "        print(\"Loading saved model...\")\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(\n",
    "                MODEL_PATH,\n",
    "                compile=False,\n",
    "                safe_mode=False,\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Older TF/Keras versions don't have safe_mode\n",
    "            model = tf.keras.models.load_model(\n",
    "                MODEL_PATH,\n",
    "                compile=False,\n",
    "            )\n",
    "\n",
    "        # Compile after loading (needed for model.evaluate)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        print(\"Loaded model from:\", MODEL_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError(             # error if model file is missing\n",
    "            f\"Model not found at: {MODEL_PATH}\\n\"\n",
    "            \"Either fix the path or set TRAIN=True to train and save the model.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and model evaluation\n",
    "Returns test loss and test accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"scores\" not in globals():\n",
    "    model.evaluate(test_ds, verbose=1)\n",
    "else:\n",
    "    print(\"✅ Using cached scores (not re-evaluating).\")\n",
    "\n",
    "print(\"metrics:\", model.metrics_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels on the test set\n",
    "Collects true labels and predicted labels so we can build a classification report and confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_list = []                      # store true labels from all batches\n",
    "y_pred_list = []                      # store predicted labels from all batches\n",
    "\n",
    "for x_batch, y_batch in test_ds:      # loop over test batches (images, labels)\n",
    "    probs = model.predict(x_batch, verbose=0)   # predicted class probabilities\n",
    "    y_pred_batch = np.argmax(probs, axis=1)     # pick class with highest probability\n",
    "\n",
    "    y_true_batch = y_batch.numpy()    # true labels as numpy\n",
    "\n",
    "    y_true_list.append(y_true_batch)  # collect true labels\n",
    "    y_pred_list.append(y_pred_batch)  # collect predicted labels\n",
    "\n",
    "y_true = np.concatenate(y_true_list)  # merge all true labels into one array\n",
    "y_pred = np.concatenate(y_pred_list)  # merge all predictions into one array\n",
    "\n",
    "print(\"Manual test accuracy:\", (y_true == y_pred).mean())  # compare with model.evaluate accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix plot\n",
    "Shows where the model confuses one class for another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # confusion matrix tools\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)      # build confusion matrix from true vs predicted labels\n",
    "\n",
    "plt.figure(figsize=(10, 8))               # set figure size\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)  # wrap matrix for plotting\n",
    "disp.plot(values_format=\"d\")              # plot counts as integers\n",
    "plt.title(\"Confusion Matrix (Test)\")      # title\n",
    "plt.show()                                # display plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report\n",
    "Precision/recall/F1-score per class, plus macro and weighted averages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cm = confusion_matrix(y_true, y_pred)  # compute confusion matrix\n",
    "\n",
    "print(classification_report(\n",
    "    y_true, y_pred,                    # true labels vs predicted labels\n",
    "    target_names=class_names,          # show class names instead of numbers\n",
    "    digits=4                           # print metrics with 4 decimals\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect wrong predictions\n",
    "Collects a few misclassified examples so you can visually check what went wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt   # plotting\n",
    "import numpy as np                # arrays\n",
    "\n",
    "wrong = []  # store (image, true_label, pred_label) for mistakes\n",
    "\n",
    "for x_batch, y_batch in test_ds:                      # loop over test batches\n",
    "    probs = model.predict(x_batch, verbose=0)         # predict probabilities\n",
    "    pred = np.argmax(probs, axis=1)                   # predicted class ids\n",
    "\n",
    "    if len(y_batch.shape) > 1 and y_batch.shape[-1] > 1:  # if labels are one-hot\n",
    "        true = np.argmax(y_batch.numpy(), axis=1)          # convert to class ids\n",
    "    else:\n",
    "        true = y_batch.numpy().astype(int)                 # sparse labels -> int\n",
    "\n",
    "    for i in range(len(true)):                             # check each item in batch\n",
    "        if true[i] != pred[i]:\n",
    "            wrong.append((x_batch[i].numpy().astype(\"uint8\"), true[i], pred[i]))  # save mistake\n",
    "    if len(wrong) >= 25:\n",
    "        break                                              # stop after collecting 25 mistakes\n",
    "\n",
    "plt.figure(figsize=(12, 10))                               # create figure\n",
    "for i, (img, t, p) in enumerate(wrong[:25]):               # plot up to 25 wrong images\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(img)                                       # show image\n",
    "    t_name = class_names[t] if \"class_names\" in globals() else str(t)  # true name\n",
    "    p_name = class_names[p] if \"class_names\" in globals() else str(p)  # pred name\n",
    "    plt.title(f\"T:{t_name}\\nP:{p_name}\", fontsize=8)       # title (T=true, P=pred)\n",
    "    plt.axis(\"off\")                                       # hide axes\n",
    "plt.tight_layout()                                        # spacing\n",
    "plt.show()                                                # display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # arrays + numerical ops\n",
    "\n",
    "pairs = []                          # store (count, true_class_name, predicted_class_name)\n",
    "n = len(class_names)                # number of classes\n",
    "\n",
    "for i in range(n):                  # loop over true class index\n",
    "    for j in range(n):              # loop over predicted class index\n",
    "        if i != j and cm[i, j] > 0: # keep only mistakes (off-diagonal) with count > 0\n",
    "            pairs.append((cm[i, j], class_names[i], class_names[j]))  # save this confusion pair\n",
    "\n",
    "for c, true_name, pred_name in sorted(pairs, reverse=True)[:15]:  # top 15 biggest confusions\n",
    "    print(f\"{c:>3}  True: {true_name:30s}  → Pred: {pred_name}\")   # print nicely formatted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main remaining errors are concentrated in a few visually similar categories—especially Spider_mites → Target_Spot (9) and Spider_mites → healthy (8), plus smaller mix-ups like Septoria_leaf_spot → Early_blight (5). This is reasonable because these classes can share similar “spotty” textures, and symptoms may be subtle or appear in early stages, making the visual cues harder even for humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
